{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a00013",
   "metadata": {},
   "source": [
    "# Dice.com Web Scraper "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdccbc7",
   "metadata": {},
   "source": [
    "Logical Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6a95d",
   "metadata": {},
   "source": [
    "<img src='Dice Scraping Logical Diagram.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498bb881",
   "metadata": {},
   "source": [
    "## Imports, Functions, and Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d3a1c",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "66008161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "575d32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_api_key = <Enter your own key here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c8b45",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "738f040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes: raw_response\n",
    "\"\"\"\n",
    "def get_job_listing(raw_response: str) -> str:\n",
    "    try:\n",
    "        offset = 15\n",
    "        job_title_index = raw_response.text.find('\"job_title\"')\n",
    "        end_job_listing_index = raw_response.text[job_title_index+offset:].find(',')\n",
    "        return raw_response.text[job_title_index+offset : job_title_index+offset+end_job_listing_index-1]\n",
    "    except:\n",
    "        print('Error in get_job_listing')\n",
    "        return\n",
    "\n",
    "def get_original_date_posted(raw_response: str) -> str:\n",
    "    try:\n",
    "        offset = 16\n",
    "        date_posted_index = raw_response.text.find('\"datePosted\"')\n",
    "        end_date_posted_index = raw_response.text[date_posted_index+offset:].find(',')\n",
    "        return raw_response.text[date_posted_index+offset : date_posted_index+offset+end_date_posted_index-1]\n",
    "    except:\n",
    "        print('Error in get_original_date_posted')\n",
    "        return\n",
    "\n",
    "def get_skills(raw_response: str) -> str:\n",
    "    try:\n",
    "        offset = 13\n",
    "        skills_index = raw_response.text.find('\"skills\"')\n",
    "        end_skills_index = raw_response.text[skills_index+offset:].find(']')\n",
    "        return raw_response.text[skills_index+offset : skills_index+offset+end_skills_index-1]\n",
    "    except:\n",
    "        print('Error in get_skills')\n",
    "        return\n",
    "    \n",
    "def get_company_name(raw_response: str) -> str:\n",
    "    try:\n",
    "        offset = 17\n",
    "        index = raw_response.text.find('\"companyName\"')\n",
    "        end_index = raw_response.text[index+offset:].find(',')\n",
    "        return raw_response.text[index+offset : index+offset+end_index-1]\n",
    "    except:\n",
    "        print('Error in get_company_name')\n",
    "        return\n",
    "\n",
    "def get_job_city(raw_response: str) -> str:\n",
    "    try:\n",
    "        offset = 14\n",
    "        index = raw_response.text.find('\"jobCity\"')\n",
    "        end_index = raw_response.text[index+offset:].find(']')\n",
    "        return raw_response.text[index+offset : index+offset+end_index-1]\n",
    "    except:\n",
    "        print('Error in get_job_city')\n",
    "        return    \n",
    "\n",
    "def get_job_region(raw_response: str) -> str:\n",
    "    try:\n",
    "        offset = 16\n",
    "        index = raw_response.text.find('\"jobRegion\"')\n",
    "        end_index = raw_response.text[index+offset:].find(']')\n",
    "        return raw_response.text[index+offset : index+offset+end_index-1]\n",
    "    except:\n",
    "        print('Error in get_job_region')\n",
    "        return  \n",
    "\n",
    "def get_job_postal_code(raw_response: str) -> str:\n",
    "    try:\n",
    "        offset = 20\n",
    "        index = raw_response.text.find('\"jobPostalCode\"')\n",
    "        end_index = raw_response.text[index+offset:].find(']')\n",
    "        return raw_response.text[index+offset : index+offset+end_index-1]\n",
    "    except:\n",
    "        print('Error in get_job_postal_code')\n",
    "        return  \n",
    "    \n",
    "\"\"\"\n",
    "Takes: unfiltered_response\n",
    "\"\"\"\n",
    "def get_intext_job_title(text: str) -> str:\n",
    "    try: \n",
    "        end_index = text.find('-') \n",
    "        return text[0:end_index-1]\n",
    "    except:\n",
    "        print('Error in get_intext_job_title')\n",
    "        return    \n",
    "    \n",
    "def get_intext_company_name(text: str) -> str:\n",
    "    try:\n",
    "        start_index = text.find('-')\n",
    "        end_index = 0\n",
    "        for char_index in range(len(text)):\n",
    "            curr_index = char_index + start_index + 1\n",
    "            if text[curr_index] == '-':\n",
    "                end_index = curr_index\n",
    "                break\n",
    "        return text[start_index+2:end_index-1]\n",
    "    except:\n",
    "        print('Error in get_intext_company_name')\n",
    "        return\n",
    "\n",
    "def get_intext_date_posted(text: str) -> str:\n",
    "    try:\n",
    "        today_date = datetime.date.today()\n",
    "        if ('hours ago' in text) | ('hour ago' in text):\n",
    "            return str(today_date)\n",
    "        if 'weeks ago' in text:\n",
    "            start_index = text.find('weeks ago')\n",
    "            ### If posting is made more than 9 weeks ago\n",
    "            if text[start_index-3].isdigit():\n",
    "                num_weeks_ago = int(text[start_index-3:start_index-1])\n",
    "            else:\n",
    "                num_weeks_ago = int(text[start_index-2])\n",
    "            threeWeeks = datetime.timedelta(weeks = num_weeks_ago)\n",
    "            return str(today_date - threeWeeks)\n",
    "    except:\n",
    "        print('Error in get_intext_date_posted')\n",
    "        return\n",
    "\n",
    "def get_intext_location(text: str) -> str:\n",
    "    try:\n",
    "        end_index = text.find('| Dice.com')\n",
    "        comp_name = get_intext_company_name(text)\n",
    "        len_company_name = len(comp_name)\n",
    "        start_index = text.find(comp_name) + len_company_name\n",
    "        for char in text[start_index:end_index]:\n",
    "            if char == '-':\n",
    "                break\n",
    "            start_index += 1\n",
    "        return text[start_index+2:end_index-1]\n",
    "    except:\n",
    "        print('Error in get_intext_location')\n",
    "        return    \n",
    "\n",
    "\"\"\"\n",
    "Takes: uncased_unfiltered_response_nocommas\n",
    "\"\"\"\n",
    "\n",
    "def get_education_level(text: str) -> str:\n",
    "    try:\n",
    "        education_levels_found = []\n",
    "\n",
    "        phd_spellings = ['pdh','doctorate']\n",
    "        masters_spellings = ['masters','ms','ma','ms/ma','ma/ms']\n",
    "        bachelor_spellings = ['bachelor','bachelors','bachelor\\'s','bs','ba','bs/ba','ba/bs']\n",
    "        associate_spellings = ['associates','associate\\'s']\n",
    "\n",
    "        # Check for Phd's\n",
    "        for phd in phd_spellings:\n",
    "            if phd in text:\n",
    "                education_levels_found.append('PhD')\n",
    "                break\n",
    "\n",
    "        # Check for masters\n",
    "        for masters in masters_spellings:\n",
    "            if masters in text:\n",
    "                education_levels_found.append('Masters')\n",
    "                break\n",
    "\n",
    "        # Check for bachelors\n",
    "        for bachelor in bachelor_spellings:\n",
    "            if bachelor in text:\n",
    "                education_levels_found.append(\"Bachelor's\")\n",
    "                break\n",
    "\n",
    "        # Check for associates\n",
    "        for associate in associate_spellings:\n",
    "            if associate in text:\n",
    "                education_levels_found.append(\"Associate's\")\n",
    "\n",
    "        if len(education_levels_found) == 0:\n",
    "            return 'NA'\n",
    "        if len(education_levels_found) == 1:\n",
    "            return education_levels_found[0]\n",
    "        else: ### If len(education_levels_found) > 1\n",
    "            ### Return a range\n",
    "            return str(education_levels_found[-1]) + ' - ' + str(education_levels_found[0])\n",
    "    except:\n",
    "        print('Error in get_education_level')\n",
    "        return           \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "General purpose method\n",
    "\"\"\"\n",
    "def get_word_occurences(text: str, text_file: str):\n",
    "    try:\n",
    "        textfile = open(text_file,'r') ### Open the list\n",
    "        raw_text = textfile.read() ### Read into a string\n",
    "        textfile.close() ### Close the file\n",
    "        raw_text = raw_text.lower() ### Lowercase all the strings\n",
    "        raw_text_list = raw_text.split('\\n') ### Convert string into list\n",
    "        ### If a programming language is in the text, add it to a dictionary\n",
    "        raw_set = set()\n",
    "        for word in raw_text_list:\n",
    "            if word in raw_set:\n",
    "                continue\n",
    "            else:\n",
    "                if text.count(' '+word+' ') > 0:\n",
    "                    raw_set.add(word)\n",
    "        try:\n",
    "            raw_set.remove('')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return str(raw_set).replace('{','').replace('}','').replace('\\'','') if len(raw_set) > 0 else None\n",
    "    except:\n",
    "        print('Error in get_word_occurences')\n",
    "        return\n",
    "        \n",
    "    \n",
    "def get_intext_skills(text: str, text_file: str):\n",
    "    return get_word_occurences(text, text_file)\n",
    "\n",
    "def get_coding_languages(text: str, text_file: str):\n",
    "    return get_word_occurences(text, text_file)\n",
    "\n",
    "def get_technologies(text: str, text_file: str):\n",
    "    return get_word_occurences(text, text_file)\n",
    "\n",
    "def get_methodologies(text: str, text_file: str):\n",
    "    return get_word_occurences(text, text_file)\n",
    "\n",
    "def get_operating_systems(text: str, text_file: str):\n",
    "    return get_word_occurences(text, text_file)\n",
    "\n",
    "def get_remote(text: str) -> bool:\n",
    "    return True if 'remote' in text else False\n",
    "\n",
    "def get_years_experience(text: str) -> str:\n",
    "    try:\n",
    "        years_list = []\n",
    "        indexes_of_occurrence = [m.start() for m in re.finditer('years', text)]\n",
    "        for index in indexes_of_occurrence:\n",
    "            #print(text[index-5:index])\n",
    "            for char in text[index-5:index]:\n",
    "                if char.isdigit():\n",
    "                    years_list.append(int(char))\n",
    "\n",
    "        if len(years_list) == 0:\n",
    "            return ''\n",
    "        if len(years_list) == 1:\n",
    "            return str(years_list[0])\n",
    "        if len(years_list) > 1:\n",
    "            return str(min(years_list)) + ' - ' + str(max(years_list))\n",
    "    except:\n",
    "        print(\"Error in get_years_experience\")\n",
    "    \n",
    "def get_all_attributes(raw_response, unfiltered_response, uncased_unfilter_response_nocomma):\n",
    "    results = []\n",
    "    ### Raw Response\n",
    "    results.append(get_job_listing(raw_response))\n",
    "    results.append(get_original_date_posted(raw_response))\n",
    "    results.append(get_skills(raw_response))\n",
    "    results.append(get_company_name(raw_response))\n",
    "    results.append(get_job_city(raw_response))\n",
    "    results.append(get_job_region(raw_response))\n",
    "    results.append(get_job_postal_code(raw_response))\n",
    "    \n",
    "    ### Unfiltered Response\n",
    "    results.append(get_intext_job_title(unfiltered_response))\n",
    "    results.append(get_intext_company_name(unfiltered_response))\n",
    "    results.append(get_intext_date_posted(unfiltered_response))\n",
    "    results.append(get_intext_location(unfiltered_response))\n",
    "    \n",
    "    ### Uncased Unfiltered\n",
    "    results.append(str(get_education_level(uncased_filtered_response_nocomma)))\n",
    "    results.append(get_intext_skills(uncased_filtered_response_nocomma, './both_skills_list.txt'))\n",
    "    results.append(get_coding_languages(uncased_filtered_response_nocomma, './coding_languages.txt'))\n",
    "    results.append(get_technologies(uncased_filtered_response_nocomma, './technologies.txt'))\n",
    "    results.append(get_methodologies(uncased_filtered_response_nocomma, './methodologies.txt'))\n",
    "    results.append(get_operating_systems(uncased_filtered_response_nocomma, './operating_systems.txt'))\n",
    "    results.append(str(get_remote(uncased_filtered_response_nocomma)))\n",
    "    results.append(str(get_years_experience(uncased_filtered_response_nocomma)))\n",
    "    \n",
    "    ### Add the date of processing\n",
    "    now = datetime.datetime.now() # current date and time\n",
    "    year = now.strftime(\"%Y\")\n",
    "    month = now.strftime(\"%m\")\n",
    "    day = now.strftime(\"%d\")\n",
    "    date = month+day+year\n",
    "    results.append(str(date))\n",
    "    \n",
    "    ### Salary (still need to implement method)\n",
    "    results.append(None)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_job_links_from_links(clean_links):\n",
    "    only_job_links = []\n",
    "    for link in clean_links:\n",
    "        result = re.search(\"^\\/jobs\\/detail\", str(link))\n",
    "        if result is not None:\n",
    "            whole_link = \"https://www.dice.com\"+link\n",
    "            only_job_links.append(whole_link)\n",
    "    return only_job_links\n",
    "\n",
    "def add_links_to_master_set(set_of_links, file_location):\n",
    "    ### Open the existing set of links\n",
    "    file = open(file_location,'rb')\n",
    "    existing_links = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    len_b4 =len(existing_links)\n",
    "    \n",
    "    ### Add the existing set with the new one\n",
    "    existing_links.update(set_of_links)\n",
    "    print(\"Number of links added: {}\".format(len(existing_links) - len_b4))\n",
    "    \n",
    "    ### Write back to pickle\n",
    "    file = open(file_location,'wb')\n",
    "    pickle.dump(existing_links, file)\n",
    "    file.close()\n",
    "    \n",
    "    return True\n",
    "\n",
    "def print_existing_links(file_location):\n",
    "    file = open(file_location,'rb')\n",
    "    existing_links = pickle.load(file)\n",
    "    file.close()\n",
    "    [print(link) for link in existing_links]\n",
    "    \n",
    "def get_existing_links(file_location):\n",
    "    file = open(file_location,'rb')\n",
    "    existing_links = pickle.load(file)\n",
    "    file.close()\n",
    "    return exisiting_links\n",
    "\n",
    "def get_responses(URL):\n",
    "    try:\n",
    "        ### Unique Key\n",
    "        payload = {'api_key': scraper_api_key, \n",
    "                   'url': URL}\n",
    "        ### Make request\n",
    "        raw_response = requests.get('http://api.scraperapi.com', params=payload)\n",
    "\n",
    "        ### If bad request, exit\n",
    "        if raw_response == 200:\n",
    "            return\n",
    "\n",
    "        ### Remove the html\n",
    "        unfiltered_response = ' '.join(str(BeautifulSoup(raw_response.text.\n",
    "                                                         replace(\">\",\"> \").\n",
    "                                                         replace(\"<\",\" <\").\n",
    "                                                         replace(\"\\n\",\"\").\n",
    "                                                         replace(\"\\t\",\"\"), 'lxml').text).split())\n",
    "\n",
    "        ### Clean the response more\n",
    "        end_characters = 'Save Create Alert'\n",
    "        end_index = unfiltered_response.find(end_characters)\n",
    "        start_characters = '(email@domain.com). Create Alert '\n",
    "        start_index = unfiltered_response.find(start_characters)\n",
    "        Cased_filtered_response = unfiltered_response[start_index+len(start_characters):end_index]\n",
    "        uncased_filtered_response = Cased_filtered_response.lower()\n",
    "        uncased_filtered_response_nocomma = uncased_filtered_response.replace(',',' ') ### Replace the comma to help with identifying skills\n",
    "        return raw_response, unfiltered_response, uncased_filtered_response_nocomma\n",
    "    except:\n",
    "        print('Error in get_responses')\n",
    "        return\n",
    "\n",
    "### For fixing any date formatting issues    \n",
    "dates_fix = {'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12}\n",
    "def fix_dates(row):\n",
    "    try:\n",
    "        temp_list = row.split('-')\n",
    "        if temp_list[1] in dates_fix.keys():\n",
    "            temp_str_1 = dates_fix.get(temp_list[1])\n",
    "            temp_str_2 = temp_list[0]\n",
    "            return str(temp_str_1) + ' - ' + str(temp_str_2)\n",
    "    except:\n",
    "        return row\n",
    "    \n",
    "### For debugging    \n",
    "def check_if_bad_link(URL):\n",
    "    return True if df_failed_urls[df_failed_urls['URL'] == URL].shape[0] > 0 else False\n",
    "    \n",
    "def add_bad_link(URL):\n",
    "    if df_failed_urls[df_failed_urls['URL'] == URL].shape[0] > 0:\n",
    "        new_val = df_failed_urls[df_failed_urls['URL'] == URL]['attemps'] + 1\n",
    "        if int(new_val) > 3:\n",
    "            index_of_url = df_failed_urls[df_failed_urls['URL'] == URL].index\n",
    "            df_failed_urls.drop(index_of_url, inplace=True)\n",
    "            df_failed_urls.reset_index(inplace=True, drop=True)\n",
    "            print('URL dropped: {}'.format(URL))\n",
    "            return\n",
    "        df_failed_urls.loc[df_failed_urls[df_failed_urls['URL'] == URL].index, 'attemps'] = new_val\n",
    "    else:\n",
    "        df_failed_urls.loc[len(df_failed_urls)] = [URL, 1]\n",
    "        \n",
    "def remove_link(URL):\n",
    "    index_of_url = df_failed_urls[df_failed_urls['URL'] == URL].index\n",
    "    df_failed_urls.drop(index_of_url, inplace=True)\n",
    "    df_failed_urls.reset_index(inplace=True, drop=True)\n",
    "    print('Removed:', URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05334b",
   "metadata": {},
   "source": [
    "**Databases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cd6483fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings = pd.read_csv('listings_attributes.csv')\n",
    "df_listings_and_urls = pd.read_csv('listings_and_urls.csv')\n",
    "df_failed_urls = pd.read_csv('failed_links_attemps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4f797c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_listing</th>\n",
       "      <th>original_date_posted</th>\n",
       "      <th>skills</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_city</th>\n",
       "      <th>job_region</th>\n",
       "      <th>job_postal_code</th>\n",
       "      <th>intext_job_title</th>\n",
       "      <th>intext_company_name</th>\n",
       "      <th>intext_date_posted</th>\n",
       "      <th>...</th>\n",
       "      <th>intext_skills</th>\n",
       "      <th>coding_languages</th>\n",
       "      <th>technologies</th>\n",
       "      <th>methodologies</th>\n",
       "      <th>operating_systems</th>\n",
       "      <th>remote</th>\n",
       "      <th>years_experience</th>\n",
       "      <th>date_of_processing</th>\n",
       "      <th>salary</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>2021-06-28T22:17:51Z</td>\n",
       "      <td>Artificial Intelligence, Python, IT, SAS, SQL,...</td>\n",
       "      <td>New York Life Insurance Company</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10001</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>New York Life Insurance Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>machine learning, model deployment, sales, tec...</td>\n",
       "      <td>sas, r, processing, spark, lasso, sql, python</td>\n",
       "      <td>NaN</td>\n",
       "      <td>incremental</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>3 - 5</td>\n",
       "      <td>6292021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Senior-Data-S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-06-01T00:32:45Z</td>\n",
       "      <td>Research, Computer, Programming, Python, Java,...</td>\n",
       "      <td>comScore</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12010</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>comScore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>machine learning, online advertising, clusteri...</td>\n",
       "      <td>scala, r, javascript, source, java, sql, python</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>6292021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-06-09T18:16:51Z</td>\n",
       "      <td>Data, collect, clean, analyze</td>\n",
       "      <td>University Of Delaware</td>\n",
       "      <td>Newark</td>\n",
       "      <td>DE</td>\n",
       "      <td>19702</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>University Of Delaware</td>\n",
       "      <td>6/15/2021</td>\n",
       "      <td>...</td>\n",
       "      <td>sas, data analysis, algorithms, r, writing, ma...</td>\n",
       "      <td>sas, r, stata, clean, sql, python</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>college</td>\n",
       "      <td>False</td>\n",
       "      <td>3 - 5</td>\n",
       "      <td>6292021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Principal Data Scientist - Search</td>\n",
       "      <td>2021-04-30T23:30:59Z</td>\n",
       "      <td>Algorithms, Engineers, Python, Java, Data Mini...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>94086</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Search</td>\n",
       "      <td>6/29/2021</td>\n",
       "      <td>...</td>\n",
       "      <td>machine learning, cadence, online advertising,...</td>\n",
       "      <td>plus, scala, r, ml, source, java, spark, python</td>\n",
       "      <td>tensorflow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>3 - 7</td>\n",
       "      <td>6292021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Principal-Dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - Entry Level</td>\n",
       "      <td>2021-05-07T00:30:18Z</td>\n",
       "      <td>Laboratory, Security, Applications, Java, Pyth...</td>\n",
       "      <td>Lawrence Livermore National Laboratory</td>\n",
       "      <td>Livermore</td>\n",
       "      <td>CA</td>\n",
       "      <td>94550</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Entry Level</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>database, machine learning, research and devel...</td>\n",
       "      <td>r, matlab, c, python, java, q, c++, processing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linux</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6292021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         job_listing  original_date_posted  \\\n",
       "0              Senior Data Scientist  2021-06-28T22:17:51Z   \n",
       "1                     Data Scientist  2021-06-01T00:32:45Z   \n",
       "2                     Data Scientist  2021-06-09T18:16:51Z   \n",
       "3  Principal Data Scientist - Search  2021-04-30T23:30:59Z   \n",
       "4       Data Scientist - Entry Level  2021-05-07T00:30:18Z   \n",
       "\n",
       "                                              skills  \\\n",
       "0  Artificial Intelligence, Python, IT, SAS, SQL,...   \n",
       "1  Research, Computer, Programming, Python, Java,...   \n",
       "2                      Data, collect, clean, analyze   \n",
       "3  Algorithms, Engineers, Python, Java, Data Mini...   \n",
       "4  Laboratory, Security, Applications, Java, Pyth...   \n",
       "\n",
       "                             company_name   job_city job_region  \\\n",
       "0         New York Life Insurance Company   New York         NY   \n",
       "1                                comScore  Amsterdam        NaN   \n",
       "2                  University Of Delaware     Newark         DE   \n",
       "3                                 Walmart  Sunnyvale         CA   \n",
       "4  Lawrence Livermore National Laboratory  Livermore         CA   \n",
       "\n",
       "  job_postal_code          intext_job_title              intext_company_name  \\\n",
       "0           10001     Senior Data Scientist  New York Life Insurance Company   \n",
       "1           12010            Data Scientist                         comScore   \n",
       "2           19702            Data Scientist           University Of Delaware   \n",
       "3           94086  Principal Data Scientist                           Search   \n",
       "4           94550            Data Scientist                      Entry Level   \n",
       "\n",
       "  intext_date_posted  ...                                      intext_skills  \\\n",
       "0                NaN  ...  machine learning, model deployment, sales, tec...   \n",
       "1                NaN  ...  machine learning, online advertising, clusteri...   \n",
       "2          6/15/2021  ...  sas, data analysis, algorithms, r, writing, ma...   \n",
       "3          6/29/2021  ...  machine learning, cadence, online advertising,...   \n",
       "4                NaN  ...  database, machine learning, research and devel...   \n",
       "\n",
       "                                  coding_languages technologies methodologies  \\\n",
       "0    sas, r, processing, spark, lasso, sql, python          NaN   incremental   \n",
       "1  scala, r, javascript, source, java, sql, python          NaN           NaN   \n",
       "2                sas, r, stata, clean, sql, python          NaN           NaN   \n",
       "3  plus, scala, r, ml, source, java, spark, python   tensorflow           NaN   \n",
       "4   r, matlab, c, python, java, q, c++, processing          NaN           NaN   \n",
       "\n",
       "  operating_systems remote years_experience  date_of_processing salary  \\\n",
       "0               NaN  False            3 - 5             6292021    NaN   \n",
       "1               NaN  False                2             6292021    NaN   \n",
       "2           college  False            3 - 5             6292021    NaN   \n",
       "3               NaN  False            3 - 7             6292021    NaN   \n",
       "4             linux  False              NaN             6292021    NaN   \n",
       "\n",
       "                                                 URL  \n",
       "0  https://www.dice.com/jobs/detail/Senior-Data-S...  \n",
       "1  https://www.dice.com/jobs/detail/Data-Scientis...  \n",
       "2  https://www.dice.com/jobs/detail/Data-Scientis...  \n",
       "3  https://www.dice.com/jobs/detail/Principal-Dat...  \n",
       "4  https://www.dice.com/jobs/detail/Data-Scientis...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a9eb383f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_url</th>\n",
       "      <th>listing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dice.com/jobs/detail/Senior-Data-S...</td>\n",
       "      <td>Senior Data Scientist - New York Life Insuranc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
       "      <td>Data Scientist - comScore - Amsterdam | Dice.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
       "      <td>Data Scientist - University Of Delaware - Newa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.dice.com/jobs/detail/Principal-Dat...</td>\n",
       "      <td>Principal Data Scientist - Search - Walmart - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
       "      <td>Data Scientist - Entry Level - Lawrence Liverm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_url  \\\n",
       "0  https://www.dice.com/jobs/detail/Senior-Data-S...   \n",
       "1  https://www.dice.com/jobs/detail/Data-Scientis...   \n",
       "2  https://www.dice.com/jobs/detail/Data-Scientis...   \n",
       "3  https://www.dice.com/jobs/detail/Principal-Dat...   \n",
       "4  https://www.dice.com/jobs/detail/Data-Scientis...   \n",
       "\n",
       "                                             listing  \n",
       "0  Senior Data Scientist - New York Life Insuranc...  \n",
       "1  Data Scientist - comScore - Amsterdam | Dice.c...  \n",
       "2  Data Scientist - University Of Delaware - Newa...  \n",
       "3  Principal Data Scientist - Search - Walmart - ...  \n",
       "4  Data Scientist - Entry Level - Lawrence Liverm...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listings_and_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "902d14dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>attemps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [URL, attemps]\n",
       "Index: []"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_failed_urls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2cae3",
   "metadata": {},
   "source": [
    "## Query Dice.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38972ac1",
   "metadata": {},
   "source": [
    "*Getting the first inital links for a '__Data Scientist__'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fc4ec524",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.dice.com/jobs/q-Data+Scientist-jobs'\n",
    "### Unique Key\n",
    "payload = {'api_key': scraper_api_key, \n",
    "           'url': URL}\n",
    "### Make request\n",
    "r = requests.get('http://api.scraperapi.com', params=payload)\n",
    "\n",
    "### Clean up the raw text\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "### Get all the links\n",
    "clean_links = []\n",
    "for link in soup.find_all('a'):\n",
    "    current_link = link.get('href')\n",
    "    if current_link not in clean_links:\n",
    "        clean_links.append(current_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f214797",
   "metadata": {},
   "source": [
    "## Getting the total number of pages w/ links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "febf8447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672\n",
      "There are 1672 pages with links\n"
     ]
    }
   ],
   "source": [
    "job_lengths_list = []\n",
    "for link in clean_links:\n",
    "    result = re.search(\"^\\/jobs\\/.*p=\", str(link)) # Starts with /jobs/, has some characters, then has p= in it\n",
    "    if result is not None:\n",
    "        whole_link = \"https://www.dice.com\"+link\n",
    "        job_lengths_list.append(whole_link)\n",
    "        \n",
    "start_index = job_lengths_list[-1].find(\"=\")\n",
    "number_of_pages = int(job_lengths_list[-1][start_index+1:])\n",
    "print('There are {} pages with links'.format(number_of_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf6fa1",
   "metadata": {},
   "source": [
    "## Query each page to get all the links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64eece",
   "metadata": {},
   "source": [
    "*Note, this may return a very lage number of pages. If you do not want to use up thousands of requests at once, add a __break__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ab8b9c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 new links found!\n",
      "116 links in set_of_links\n"
     ]
    }
   ],
   "source": [
    "### Create a set of links to run\n",
    "set_of_links = set()\n",
    "### get the URL's that have already been used\n",
    "used_URLs = set(df_listings_and_urls['source_url'])\n",
    "failed_URLs = df_failed_urls['URL'].tolist()\n",
    "\n",
    "new_links_found = 0\n",
    "offset = 270\n",
    "\n",
    "for i in range(number_of_pages-offset):\n",
    "    URL = 'https://www.dice.com/jobs/q-Data+Scientist-jobs?p=' + str(i+offset)\n",
    "    \n",
    "    ## Unique Key\n",
    "    payload = {'api_key': scraper_api_key, \n",
    "               'url': URL}\n",
    "    ### Make request\n",
    "    r = requests.get('http://api.scraperapi.com', params=payload)\n",
    "\n",
    "    ### Clean up the raw text\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "    ### Get all the links\n",
    "    clean_links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        current_link = link.get('href')\n",
    "        if current_link not in clean_links:\n",
    "            clean_links.append(current_link)\n",
    "            \n",
    "    ### Filter the links more\n",
    "    all_links_to_scrape = get_job_links_from_links(clean_links)\n",
    "    \n",
    "    \"\"\" \n",
    "    ####################################################\n",
    "    Check to make sure the link hasn't been used already\n",
    "    ####################################################\n",
    "    \"\"\"   \n",
    "    for link in all_links_to_scrape:\n",
    "        \n",
    "        ### Check to make sure the link doesn't already exist in the database\n",
    "        if link in used_URLs:\n",
    "            print('Link already used:', link)\n",
    "            continue\n",
    "            \n",
    "        ### Check to make sure the link isn't already a bad link   \n",
    "        if link in failed_URLs:\n",
    "            print('Link is marked as a bad link already', link)\n",
    "            continue\n",
    "            \n",
    "        ### Add each link to a set to prevent redudancy\n",
    "        set_of_links.add(link)\n",
    "        new_links_found += 1\n",
    "        \n",
    "    \"\"\"\n",
    "    Add an optional break down here\n",
    "    \"\"\"\n",
    "    if i == 5: ### Will capture around 120 links\n",
    "        break\n",
    "    else: \n",
    "        continue\n",
    "        \n",
    "print('{} new links found!'.format(new_links_found))\n",
    "print('{} links in set_of_links'.format(len(set_of_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fec161",
   "metadata": {},
   "source": [
    "## Run the web scraper over each link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f5a65325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 links added!\n"
     ]
    }
   ],
   "source": [
    "failed_links = []\n",
    "failed_listing_counter = 0\n",
    "num_links_added = 0\n",
    "\n",
    "for job_link in list(set_of_links):    \n",
    "    try:\n",
    "        ### Get the listing\n",
    "        raw_response, unfiltered_response, uncased_filtered_response_nocomma = get_responses(job_link)\n",
    "        if '404 Not Found' in unfiltered_response:\n",
    "            ### Add the bad response to list of links tried\n",
    "            add_bad_link(job_link)\n",
    "            print(\"404 Not Found\")\n",
    "                  \n",
    "            failed_listing_coutner += 1\n",
    "            if failed_listing_counter == 3:\n",
    "                print(\"Cooling down for 10 seconds before continuing...\")\n",
    "                ### Should allow for a proxy to be reset (theoretically)\n",
    "                time.sleep(10)\n",
    "                failed_listing_coutner = 0\n",
    "            continue\n",
    "            \n",
    "        ### Get the attributes\n",
    "        results = get_all_attributes(raw_response, unfiltered_response, uncased_filtered_response_nocomma)\n",
    "        ### Add url to results\n",
    "        results += [job_link]\n",
    "        ### Add the attributes to the databases\n",
    "        df_listings.loc[len(df_listings)] = results\n",
    "        ### Add the unfiltered_response to a seperate db\n",
    "        df_listings_and_urls.loc[len(df_listings_and_urls)] = [job_link, unfiltered_response]\n",
    "        \n",
    "        num_links_added += 1\n",
    "    except:\n",
    "        print('Bad Listing')\n",
    "        continue\n",
    "        \n",
    "### Save the results\n",
    "df_listings.to_csv('listings_attributes.csv', index=False)\n",
    "df_listings_and_urls.to_csv('listings_and_urls.csv', index=False)\n",
    "df_failed_urls.to_csv('failed_links_attemps.csv', index=False)\n",
    "\n",
    "### Save to pickles\n",
    "file = open('listings_attributes.pkl','wb')\n",
    "pickle.dump(df_listings, file)\n",
    "file.close()\n",
    "\n",
    "file = open('raw_listings_and_urls.pkl','wb')\n",
    "pickle.dump(df_listings_and_urls, file)\n",
    "file.close()\n",
    "\n",
    "file = open('failed_links_attemps.pkl','wb')\n",
    "pickle.dump(df_failed_urls, file)\n",
    "file.close()\n",
    "\n",
    "print('{} links added!'.format(num_links_added))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1b526eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_listing</th>\n",
       "      <th>original_date_posted</th>\n",
       "      <th>skills</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_city</th>\n",
       "      <th>job_region</th>\n",
       "      <th>job_postal_code</th>\n",
       "      <th>intext_job_title</th>\n",
       "      <th>intext_company_name</th>\n",
       "      <th>intext_date_posted</th>\n",
       "      <th>...</th>\n",
       "      <th>intext_skills</th>\n",
       "      <th>coding_languages</th>\n",
       "      <th>technologies</th>\n",
       "      <th>methodologies</th>\n",
       "      <th>operating_systems</th>\n",
       "      <th>remote</th>\n",
       "      <th>years_experience</th>\n",
       "      <th>date_of_processing</th>\n",
       "      <th>salary</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>2021-06-28T22:17:51Z</td>\n",
       "      <td>Artificial Intelligence, Python, IT, SAS, SQL,...</td>\n",
       "      <td>New York Life Insurance Company</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10001</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>New York Life Insurance Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>machine learning, model deployment, sales, tec...</td>\n",
       "      <td>sas, r, processing, spark, lasso, sql, python</td>\n",
       "      <td>NaN</td>\n",
       "      <td>incremental</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>3 - 5</td>\n",
       "      <td>6292021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Senior-Data-S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-06-01T00:32:45Z</td>\n",
       "      <td>Research, Computer, Programming, Python, Java,...</td>\n",
       "      <td>comScore</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12010</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>comScore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>machine learning, online advertising, clusteri...</td>\n",
       "      <td>scala, r, javascript, source, java, sql, python</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>6292021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-06-09T18:16:51Z</td>\n",
       "      <td>Data, collect, clean, analyze</td>\n",
       "      <td>University Of Delaware</td>\n",
       "      <td>Newark</td>\n",
       "      <td>DE</td>\n",
       "      <td>19702</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>University Of Delaware</td>\n",
       "      <td>6/15/2021</td>\n",
       "      <td>...</td>\n",
       "      <td>sas, data analysis, algorithms, r, writing, ma...</td>\n",
       "      <td>sas, r, stata, clean, sql, python</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>college</td>\n",
       "      <td>False</td>\n",
       "      <td>3 - 5</td>\n",
       "      <td>6292021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Principal Data Scientist - Search</td>\n",
       "      <td>2021-04-30T23:30:59Z</td>\n",
       "      <td>Algorithms, Engineers, Python, Java, Data Mini...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>94086</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Search</td>\n",
       "      <td>6/29/2021</td>\n",
       "      <td>...</td>\n",
       "      <td>machine learning, cadence, online advertising,...</td>\n",
       "      <td>plus, scala, r, ml, source, java, spark, python</td>\n",
       "      <td>tensorflow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>3 - 7</td>\n",
       "      <td>6292021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Principal-Dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - Entry Level</td>\n",
       "      <td>2021-05-07T00:30:18Z</td>\n",
       "      <td>Laboratory, Security, Applications, Java, Pyth...</td>\n",
       "      <td>Lawrence Livermore National Laboratory</td>\n",
       "      <td>Livermore</td>\n",
       "      <td>CA</td>\n",
       "      <td>94550</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Entry Level</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>database, machine learning, research and devel...</td>\n",
       "      <td>r, matlab, c, python, java, q, c++, processing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linux</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6292021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>Talend Developer</td>\n",
       "      <td>2021-06-03T18:32:35Z</td>\n",
       "      <td>Apache HBase, Apache Hadoop, Apache Hive, Apac...</td>\n",
       "      <td>Wipro Ltd.</td>\n",
       "      <td>West Lake Hills</td>\n",
       "      <td>TX</td>\n",
       "      <td>78746</td>\n",
       "      <td>Talend Developer</td>\n",
       "      <td>Wipro Ltd.</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>...</td>\n",
       "      <td>oracle, apache hive, hdfs, pl/sql, talend, pyt...</td>\n",
       "      <td>oracle, pl/sql, spark, sql, python</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>hive</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>06302021</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Talend-Develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>Principal Machine Learning Engineer</td>\n",
       "      <td>2021-06-11T22:05:05Z</td>\n",
       "      <td>Analytics, Apache HTTP Server, Apache Lucene, ...</td>\n",
       "      <td>Inspire Recruitment Inc.</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>73301</td>\n",
       "      <td>Principal Machine Learning Engineer</td>\n",
       "      <td>Inspire Recruitment Inc.</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>...</td>\n",
       "      <td>web services, computer science, java, wins, da...</td>\n",
       "      <td>java, pipelines, rest, python, spark, sql, ml,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>vision</td>\n",
       "      <td>False</td>\n",
       "      <td>0 - 2</td>\n",
       "      <td>06302021</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Principal-Mac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>ETL/Hadoop Consultant</td>\n",
       "      <td>2021-06-28T17:15:37Z</td>\n",
       "      <td>ETL Data Engineer, Hadoop, Teradata DB2 Oracle...</td>\n",
       "      <td>Cyma Systems Inc</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>30301</td>\n",
       "      <td>ETL/Hadoop Consultant</td>\n",
       "      <td>Cyma Systems Inc</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>oracle, batch processing, root cause analysis,...</td>\n",
       "      <td>oracle, source, db2, sr, shell, spark, sql, pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>hive, root</td>\n",
       "      <td>False</td>\n",
       "      <td>3 - 8</td>\n",
       "      <td>06302021</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.dice.com/jobs/detail/ETL%26%2347Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>AIP &amp; MIP Architect</td>\n",
       "      <td>2021-06-13T19:05:57Z</td>\n",
       "      <td>Architecture, Best practices, Compliance, Cons...</td>\n",
       "      <td>Wipro Ltd.</td>\n",
       "      <td>Foster City</td>\n",
       "      <td>CA</td>\n",
       "      <td>94404</td>\n",
       "      <td>AIP &amp; MIP Architect</td>\n",
       "      <td>Wipro Ltd.</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>...</td>\n",
       "      <td>microsoft windows azure, saas, help desk, info...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0 - 5</td>\n",
       "      <td>06302021</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.dice.com/jobs/detail/AIP-%26-MIP-A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>Sr. ETL Informatica Developer</td>\n",
       "      <td>2021-06-15T22:05:26Z</td>\n",
       "      <td>ETL, Informatica, SQL, Microsoft SQL Server, D...</td>\n",
       "      <td>Peterson Technology Partners</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60290</td>\n",
       "      <td>Sr. ETL Informatica Developer</td>\n",
       "      <td>Peterson Technology Partners</td>\n",
       "      <td>2021-06-16</td>\n",
       "      <td>...</td>\n",
       "      <td>erwin, informatica, information management, mo...</td>\n",
       "      <td>sql, processing, microsoft sql server</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0 - 3</td>\n",
       "      <td>06302021</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.dice.com/jobs/detail/Sr.-ETL-Infor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>633 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             job_listing  original_date_posted  \\\n",
       "0                  Senior Data Scientist  2021-06-28T22:17:51Z   \n",
       "1                         Data Scientist  2021-06-01T00:32:45Z   \n",
       "2                         Data Scientist  2021-06-09T18:16:51Z   \n",
       "3      Principal Data Scientist - Search  2021-04-30T23:30:59Z   \n",
       "4           Data Scientist - Entry Level  2021-05-07T00:30:18Z   \n",
       "..                                   ...                   ...   \n",
       "628                     Talend Developer  2021-06-03T18:32:35Z   \n",
       "629  Principal Machine Learning Engineer  2021-06-11T22:05:05Z   \n",
       "630                ETL/Hadoop Consultant  2021-06-28T17:15:37Z   \n",
       "631                  AIP & MIP Architect  2021-06-13T19:05:57Z   \n",
       "632        Sr. ETL Informatica Developer  2021-06-15T22:05:26Z   \n",
       "\n",
       "                                                skills  \\\n",
       "0    Artificial Intelligence, Python, IT, SAS, SQL,...   \n",
       "1    Research, Computer, Programming, Python, Java,...   \n",
       "2                        Data, collect, clean, analyze   \n",
       "3    Algorithms, Engineers, Python, Java, Data Mini...   \n",
       "4    Laboratory, Security, Applications, Java, Pyth...   \n",
       "..                                                 ...   \n",
       "628  Apache HBase, Apache Hadoop, Apache Hive, Apac...   \n",
       "629  Analytics, Apache HTTP Server, Apache Lucene, ...   \n",
       "630  ETL Data Engineer, Hadoop, Teradata DB2 Oracle...   \n",
       "631  Architecture, Best practices, Compliance, Cons...   \n",
       "632  ETL, Informatica, SQL, Microsoft SQL Server, D...   \n",
       "\n",
       "                               company_name         job_city job_region  \\\n",
       "0           New York Life Insurance Company         New York         NY   \n",
       "1                                  comScore        Amsterdam        NaN   \n",
       "2                    University Of Delaware           Newark         DE   \n",
       "3                                   Walmart        Sunnyvale         CA   \n",
       "4    Lawrence Livermore National Laboratory        Livermore         CA   \n",
       "..                                      ...              ...        ...   \n",
       "628                              Wipro Ltd.  West Lake Hills         TX   \n",
       "629                Inspire Recruitment Inc.           Austin         TX   \n",
       "630                        Cyma Systems Inc          Atlanta         GA   \n",
       "631                              Wipro Ltd.      Foster City         CA   \n",
       "632            Peterson Technology Partners          Chicago         IL   \n",
       "\n",
       "    job_postal_code                     intext_job_title  \\\n",
       "0             10001                Senior Data Scientist   \n",
       "1             12010                       Data Scientist   \n",
       "2             19702                       Data Scientist   \n",
       "3             94086             Principal Data Scientist   \n",
       "4             94550                       Data Scientist   \n",
       "..              ...                                  ...   \n",
       "628           78746                     Talend Developer   \n",
       "629           73301  Principal Machine Learning Engineer   \n",
       "630           30301                ETL/Hadoop Consultant   \n",
       "631           94404                  AIP & MIP Architect   \n",
       "632           60290        Sr. ETL Informatica Developer   \n",
       "\n",
       "                 intext_company_name intext_date_posted  ...  \\\n",
       "0    New York Life Insurance Company                NaN  ...   \n",
       "1                           comScore                NaN  ...   \n",
       "2             University Of Delaware          6/15/2021  ...   \n",
       "3                             Search          6/29/2021  ...   \n",
       "4                        Entry Level                NaN  ...   \n",
       "..                               ...                ...  ...   \n",
       "628                       Wipro Ltd.         2021-06-09  ...   \n",
       "629         Inspire Recruitment Inc.         2021-06-16  ...   \n",
       "630                 Cyma Systems Inc               None  ...   \n",
       "631                       Wipro Ltd.         2021-06-16  ...   \n",
       "632     Peterson Technology Partners         2021-06-16  ...   \n",
       "\n",
       "                                         intext_skills  \\\n",
       "0    machine learning, model deployment, sales, tec...   \n",
       "1    machine learning, online advertising, clusteri...   \n",
       "2    sas, data analysis, algorithms, r, writing, ma...   \n",
       "3    machine learning, cadence, online advertising,...   \n",
       "4    database, machine learning, research and devel...   \n",
       "..                                                 ...   \n",
       "628  oracle, apache hive, hdfs, pl/sql, talend, pyt...   \n",
       "629  web services, computer science, java, wins, da...   \n",
       "630  oracle, batch processing, root cause analysis,...   \n",
       "631  microsoft windows azure, saas, help desk, info...   \n",
       "632  erwin, informatica, information management, mo...   \n",
       "\n",
       "                                      coding_languages technologies  \\\n",
       "0        sas, r, processing, spark, lasso, sql, python          NaN   \n",
       "1      scala, r, javascript, source, java, sql, python          NaN   \n",
       "2                    sas, r, stata, clean, sql, python          NaN   \n",
       "3      plus, scala, r, ml, source, java, spark, python   tensorflow   \n",
       "4       r, matlab, c, python, java, q, c++, processing          NaN   \n",
       "..                                                 ...          ...   \n",
       "628                 oracle, pl/sql, spark, sql, python         None   \n",
       "629  java, pipelines, rest, python, spark, sql, ml,...         None   \n",
       "630  oracle, source, db2, sr, shell, spark, sql, pr...         None   \n",
       "631                                               None         None   \n",
       "632              sql, processing, microsoft sql server         None   \n",
       "\n",
       "    methodologies operating_systems remote years_experience  \\\n",
       "0     incremental               NaN  False            3 - 5   \n",
       "1             NaN               NaN  False                2   \n",
       "2             NaN           college  False            3 - 5   \n",
       "3             NaN               NaN  False            3 - 7   \n",
       "4             NaN             linux  False              NaN   \n",
       "..            ...               ...    ...              ...   \n",
       "628          None              hive  False                6   \n",
       "629          None            vision  False            0 - 2   \n",
       "630          None        hive, root  False            3 - 8   \n",
       "631          None              None  False            0 - 5   \n",
       "632          None              None  False            0 - 3   \n",
       "\n",
       "    date_of_processing salary  \\\n",
       "0              6292021    NaN   \n",
       "1              6292021    NaN   \n",
       "2              6292021    NaN   \n",
       "3              6292021    NaN   \n",
       "4              6292021    NaN   \n",
       "..                 ...    ...   \n",
       "628           06302021   None   \n",
       "629           06302021   None   \n",
       "630           06302021   None   \n",
       "631           06302021   None   \n",
       "632           06302021   None   \n",
       "\n",
       "                                                   URL  \n",
       "0    https://www.dice.com/jobs/detail/Senior-Data-S...  \n",
       "1    https://www.dice.com/jobs/detail/Data-Scientis...  \n",
       "2    https://www.dice.com/jobs/detail/Data-Scientis...  \n",
       "3    https://www.dice.com/jobs/detail/Principal-Dat...  \n",
       "4    https://www.dice.com/jobs/detail/Data-Scientis...  \n",
       "..                                                 ...  \n",
       "628  https://www.dice.com/jobs/detail/Talend-Develo...  \n",
       "629  https://www.dice.com/jobs/detail/Principal-Mac...  \n",
       "630  https://www.dice.com/jobs/detail/ETL%26%2347Ha...  \n",
       "631  https://www.dice.com/jobs/detail/AIP-%26-MIP-A...  \n",
       "632  https://www.dice.com/jobs/detail/Sr.-ETL-Infor...  \n",
       "\n",
       "[633 rows x 22 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0c9e8",
   "metadata": {},
   "source": [
    "## Run the failed links again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "507cf4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link has been added! https://www.dice.com/jobs/detail/Junior-Natural-Language-Processing-Data-Scientist-Leidos-Reston-VA-20170/SCNCAPI2/R%26%234500059393\n",
      "Link has been added! https://www.dice.com/jobs/detail/Google-Cloud-Platform-Senior-Cloud-Engineer-Softpath-System%2C-LLC.---/softpath/7019333\n"
     ]
    }
   ],
   "source": [
    "failed_URLs = df_failed_urls['URL'].tolist()\n",
    "\n",
    "failed_links = []\n",
    "failed_listing_counter = 0\n",
    "\n",
    "for job_link in list(failed_URLs):    \n",
    "    try:\n",
    "        ### Get the listing\n",
    "        raw_response, unfiltered_response, uncased_filtered_response_nocomma = get_responses(job_link)\n",
    "        if '404 Not Found' in unfiltered_response:\n",
    "            ### Add the bad response to list of links tried\n",
    "            add_bad_link(job_link)\n",
    "            print(\"404 Not Found\", job_link)\n",
    "                  \n",
    "            failed_listing_coutner += 1\n",
    "            if failed_listing_counter == 3:\n",
    "                print(\"Cooling down for 10 seconds before continuing...\")\n",
    "                time.sleep(10)\n",
    "                failed_listing_coutner = 0\n",
    "            continue\n",
    "            \n",
    "        ### Get the attributes\n",
    "        results = get_all_attributes(raw_response, unfiltered_response, uncased_filtered_response_nocomma)\n",
    "        ### Add url to results\n",
    "        results += [job_link]\n",
    "        ### Add the attributes to the databases\n",
    "        df_listings.loc[len(df_listings)] = results\n",
    "        ### Add the unfiltered_response to a seperate db\n",
    "        df_listings_and_urls.loc[len(df_listings_and_urls)] = [job_link, unfiltered_response]\n",
    "        print('Link has been added!', job_link)\n",
    "        remove_link(job_link)\n",
    "    except:\n",
    "        print('Bad Listing')\n",
    "        continue\n",
    "        \n",
    "### Save the results\n",
    "df_listings.to_csv('listings_attributes.csv', index=False)\n",
    "df_listings_and_urls.to_csv('listings_and_urls.csv', index=False)\n",
    "df_failed_urls.to_csv('failed_links_attemps.csv', index=False)\n",
    "\n",
    "### Save to pickles\n",
    "file = open('listings_attributes.pkl','wb')\n",
    "pickle.dump(df_listings, file)\n",
    "file.close()\n",
    "\n",
    "file = open('raw_listings_and_urls.pkl','wb')\n",
    "pickle.dump(df_listings_and_urls, file)\n",
    "file.close()\n",
    "\n",
    "file = open('failed_links_attemps.pkl','wb')\n",
    "pickle.dump(df_failed_urls, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
